---
title: "Final project"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(readr)
library(tidyverse)
library(caret)
library(ROCR)
library(MLmetrics)
library(mltools)
library(rpart.plot)
library(Hmisc)
library(corrplot)
library(randomForest)

```
importing datasets
```{r}
#https://www.kaggle.com/datasets/patelprashant/employee-attrition
attrition <- read.csv('/Users/peterlayne/Downloads/WA_Fn-UseC_-HR-Employee-Attrition 2.csv')
str(attrition)
```

```{r}
char <- names(select_if(attrition, is.character))
attrition[char] <- lapply(attrition[char], as.factor)
str(attrition)
table(attrition$JobRole)
intstofac <- c("Education", "EnvironmentSatisfaction", "JobInvolvement", "JobLevel", "JobSatisfaction", "PerformanceRating", "RelationshipSatisfaction", "StockOptionLevel", "WorkLifeBalance")
attrition[intstofac] <- lapply(attrition[intstofac], as.factor)
str(attrition)
attrition$PercentSalaryHike <- attrition$PercentSalaryHike/100
str(attrition)
#Dropping employee count, StandardHours as it is all the same values
attrition <- select(attrition,-c("EmployeeCount"))
attrition <- select(attrition,-c("StandardHours"))
```

#WHEN WE WANT TO NORMALIZE HERE IS THE CODE.
attrition_normalize <- preProcess(attrition, method=c("center","scale"))
norm1 <- predict(attrition_normalize, attrition)
summary(norm1)
str(norm1)

```{r}
#Generating corrplot
#generating df of only numeric values
numerics <- names(select_if(attrition, is.numeric))
attrition_numeric <- attrition[numerics]
corr_matrix_attrition <- cor(attrition_numeric)
corrplot(corr_matrix_attrition, method ="number")
```
#Partitioning the Data
```{r}
str(attrition)
set.seed(1999)
attr_index_1 <- caret::createDataPartition(attrition$Attrition,
                                           times=1,
                                           p = 0.70,
                                           groups=1,
                                           list=FALSE)

train <- attrition[attr_index_1, ]
tune_and_test <- attrition[-attr_index_1, ]
train

#Using the function again to create the tuning set
tune_and_test_index <- createDataPartition(tune_and_test$Attrition,
                                           p = .5,
                                           list = FALSE,
                                           times = 1)

tune <- tune_and_test[tune_and_test_index, ]
test <- tune_and_test[-tune_and_test_index, ]


dim(train)
dim(test) 
dim(tune)
```
#Building and Visualizing the Basic Decision Tree
```{r}
str(train)
features <- train[,-2]#dropping Attrition because it's target variable. 
View(features)
target_var <- train$Attrition 

str(features)
str(target_var)

fitControl <- trainControl(method = "repeatedcv",
                          number = 10,
                          repeats = 5,
                          classProbs = TRUE,
                          summaryFunction = twoClassSummary) 

set.seed(1985)
attrition_model <- train(x=features,
                          y=target_var,
                          method='rpart2',
                          trControl=fitControl,
                          metric='ROC')

attrition_model
varImp(attrition_model)
attrition_model$results
plot(attrition_model)
rpart.plot(attrition_model$finalModel, type=4, extra=101)
```
#Adjusting the Treshold
```{r}
plot(density(attr_pred_tune$Yes))
adjust_thres <- function(x, y, z) {
  #x=pred_probabilities, y=threshold, z=tune_outcome
  thres <- as.factor(ifelse(x > y, "Yes", "No"))
  confusionMatrix(thres, z, positive = "Yes", dnn=c("Prediction", "Actual"), mode = "everything")
}

adjust_thres(attr_pred_tune$Yes,y=.8,tune$Attrition) #.8 looks better

adjust_thres(attr_pred_tune$Yes,y=.4,tune$Attrition)

```
#Expanding the simple decision tree grid
```{r}
tree.grid <- expand.grid(maxdepth=c(5,7,9,11))
set.seed(1984)
attrition_model_1 <- train(x=features,
                     y=target_var,
                     method="rpart2",
                     trControl=fitControl,
                     tuneGrid=tree.grid,
                     metric="ROC")
attrition_model
attrition_model_1
str(attrition)
#the expanded grid doesn't really help, I'll keep a max depth of 5
```
#Analyzing the simple decision tree
```{r}
attrition_predictions <- predict(attrition_model,test,"raw",test$Attrition)
view(attrition_predictions)

attrition_data_eval <- caret::confusionMatrix(attrition_predictions, 
                                              as.factor(test$Attrition),
                                              dnn=c("Prediction","Actual"),
                                              positive="Yes",
                                              mode = "everything"
                                              )
attrition_data_eval
```
#Random Forest
```{r}
mytry_tune <- function(x){
  xx <- dim(x)[2]-1
  sqrt(xx)
}


mytry_tune(attrition) #6


attrition_RF = randomForest(as.factor(Attrition)~.,
                            train,
                            ntree = 1000,       
                            mtry = 6,         
                            replace = TRUE,     
                            sampsize = 100,     
                            nodesize = 5,       
                            importance = TRUE,   
                            proximity = FALSE,    
                            norm.votes = TRUE,  
                            do.trace = TRUE,     
                            keep.forest = TRUE, 
                            keep.inbag = TRUE) 
attrition_RF

#Evaluating on test data
attrition_test_predict = predict(attrition_RF,
                                 test,
                                 type ="response",
                                 predict.all = TRUE)
str(attrition_test_predict)
confusionMatrix(as.factor(attrition_test_predict$aggregate),as.factor(test$Attrition), positive="1",
                dnn=c("Prediction","Actual"), mode ="everything")
```

